{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical assignment\n",
    "\n",
    "The Microsoft Research Sentence Completion Challenge (Zweig and Burges, 2011) requires a system to\n",
    "be able to predict which is the most likely word (from a set of 5 possibilities) to complete a sentence. In\n",
    "the labs you have evaluated using unigram and bigram models. In this assignment you are expected to\n",
    "investigate at least 2 extensions or alternative approaches to making predictions. Your solution does\n",
    "not need to be novel. You might choose to investigate 2 of the following approaches or 1 of the following\n",
    "approaches and 1 of your own devising.\n",
    "\n",
    "•Tri-gram (or even quadrigram) models\n",
    "•Word similarity methods e.g., using Googlenews vectors or WordNet?\n",
    "•Combining n-gram methods with word similarity methods e.g., distributional smoothing?\n",
    "•Using a neural language model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os,random,math\n",
    "import sys\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "\n",
    "training_dir=\"D:/Documents/Computer Science/Year 3/Semester Two/ANLE/lab2resources/lab2resources\"+\"/sentence-completion/Holmes_Training_Data/\"\n",
    "#testing_dir=\"D:/Documents/Computer Science/Year 3/Semester Two/ANLE/lab2resources/lab2resources/sentence-completion/testing_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by calling in needed libraries and setting the directories of data. We next want to read the data and format it correctly.\n",
    "\n",
    "The files are split between training and testing data from the holmes training data set. We then gather every line in the file within two large string array for training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 522 files in the training directory: D:/Documents/Computer Science/Year 3/Semester Two/ANLE/lab2resources/lab2resources/sentence-completion/Holmes_Training_Data\n",
      "CUBRK10.TXT FLIRT10.TXT\n",
      "['*******the project gutenberg etext of tales and fantasies******* #18 in our series by robert louis stevenson  copyright laws are changing all over the world, be sure to check the copyright laws for your country before posting these files', '', ' please take a look at the important information in this header', ' we encourage you to keep this file on your own disk, keeping an electronic path open for the next readers', ' do not remove this', '  **welcome to the world of free plain vanilla electronic texts** **etexts readable by both humans and by computers, since 1971** *these etexts prepared by hundreds of volunteers and donations* information on contacting project gutenberg to get etexts, and further information is included below', ' we need your donations', '  tales and fantasies by robert louis stevenson february, 1995 [etext #426]  *******the project gutenberg etext of tales and fantasies******* *****this file should be named tlfns10', 'txt or tlfns10', 'zip****** corrected editions of our etexts get a new number, tlfns11'] ['the project gutenberg etext of the cash boy by horatio alger jr', ' please take a look at the important information in this header', ' we encourage you to keep this file on your own disk, keeping an electronic path open for the next readers', ' do not remove this', '  **welcome to the world of free plain vanilla electronic texts** **etexts readable by both humans and by computers, since 1971** *these etexts prepared by hundreds of volunteers and donations* information on contacting project gutenberg to get etexts, and further information is included below', ' we need your donations', '  the cash boy by horatio alger jr', ' july, 1995 [etext #296]  the project gutenberg etext of the cash boy by horatio alger jr', ' *****this file should be named cashb10', 'txt or cashb10']\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR=os.path.dirname(training_dir) #this needs to be the parent directory for the training corpus\n",
    "def get_training_testing(training_dir=TRAINING_DIR,split=0.5):\n",
    "    filenames=os.listdir(training_dir)\n",
    "    n=len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
    "    #random.seed(53) #if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index=int(n*split)\n",
    "    return(filenames[:index],filenames[index:])\n",
    "\n",
    "def readWords(files):\n",
    "    training=[]\n",
    "    for afile in files: #look through each file\n",
    "                #print(\"Processing {}\".format(afile))\n",
    "                try:\n",
    "                    sent=\"\"\n",
    "                    with open(os.path.join(training_dir,afile)) as instream: #get each line and preprocess\n",
    "                        for line in instream:\n",
    "                            line=line.lower()\n",
    "                            line=line.rstrip()\n",
    "                            sent+=line+\" \" #gather each line in the array\n",
    "                    \n",
    "                except UnicodeDecodeError:\n",
    "                    print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "                except PermissionError:\n",
    "                    print(\"denied\")\n",
    "    sent=sent.replace(\"?\",\".\").replace(\"!\",\".\").replace(\"  \",\" \")\n",
    "    training=sent.split(\".\") #get array of sentences\n",
    "    try: training.remove(\" \")  #remove spaces\n",
    "    except ValueError: pass\n",
    "    try: training.remove(' m') #remove random characters\n",
    "    except ValueError: pass\n",
    "    return training\n",
    "train,test=get_training_testing() #get the data\n",
    "\n",
    "print(train[0],test[0])\n",
    "\n",
    "train_words=readWords(train) #get the train sentences\n",
    "test_words=readWords(test) #get the test sentences\n",
    "print(train_words[0:10],test_words[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply preprocessing techniques on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def get_word_list(train):\n",
    "    dict={}\n",
    "    for sent in train:\n",
    "        for word in sent.split():\n",
    "            dict[word]=0\n",
    "    return list(dict.keys()) #list of unique words return\n",
    "def convert_to_vector(words):\n",
    "    oov_token = \"<OOV>\"\n",
    "    tokenizer = Tokenizer(num_words=len(words), oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(words)\n",
    "    #word_index = tokenizer.word_index\n",
    "    sequences = tokenizer.texts_to_sequences(words)\n",
    "    return list(pad_sequences(sequences, truncating='post', maxlen=4))\n",
    "def count_freq(train):\n",
    "    for sent in train:\n",
    "        pass\n",
    "words_train=get_word_list(train_words)\n",
    "x=convert_to_vector(words_train)\n",
    "\n",
    "def get_label_as_data(labels):\n",
    "    x=len(labels) #get the size of the array\n",
    "    arr=np.zeros((x)) #create an array of empty\n",
    "    lab=[]\n",
    "    for i in range(x):\n",
    "        a=np.copy(arr)\n",
    "        a[i]=1\n",
    "        lab.append(a)\n",
    "    return np.array(lab)\n",
    "INP_SIZE=len(x[0])\n",
    "\n",
    "x[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the first model\n",
    "\n",
    "A recurrent neural network is a good choice for this sort of task as the architecture makes it well suited to sequencing. This approach is taken with stochastic gradient descsent as stated in the research paper 'Character-Aware Neural Language Models'.\n",
    "\n",
    "The second approach used a feed-forward network where the vector as input and vector prediction as output. It encountered problems where the gradient optimization would grow infinitely smaller and eventually be an unrecognized value. Sigmoid was the only activation function that prevented this, however would force values between 0 and 1 resulting in incorrect predictions for a population of training data. \n",
    "\n",
    "The next model used labeled representation to represent the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dexter Shepherd\\AppData\\Local\\Temp\\ipykernel_21488\\646122950.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y=torch.tensor(y,requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing network @ epoch 0\n",
      "Error: 93.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dexter Shepherd\\AppData\\Local\\Temp\\ipykernel_21488\\646122950.py:66: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if act==label:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "class R_Network:\n",
    "    def __init__(self,wordList,inp=4,hid=4,out=4,batch_size=1):\n",
    "        self.rnn = torch.nn.RNN(inp,hid,out)\n",
    "        self.h0 = torch.nn.Parameter(torch.randn(inp, 1, hid)) #output hidden\n",
    "        self.batch_size=batch_size\n",
    "        self.words=[\"word\" for i in range(batch_size)] #initialize empty to prevent errors\n",
    "        self.wordList=wordList\n",
    "        self.inp=inp\n",
    "        self.words={}\n",
    "        vecs=convert_to_vector(wordList)\n",
    "        for i,word in enumerate(wordList): #create dict\n",
    "            self.words[word]=vecs[i]\n",
    "    def train(self,words,targets,epochs=500):\n",
    "        #get each word in index\n",
    "        words_train=get_word_list(words)\n",
    "        #vectorize words\n",
    "        words=convert_to_vector(words_train)\n",
    "        #get words in relation of neigbours\n",
    "        #train word to next word\n",
    "        optimizer = torch.optim.SGD((self.rnn.parameters()), lr=0.20) #, momentum=0.9\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        for epoch in range(epochs):\n",
    "            error=0\n",
    "            for i in range(len(words)):\n",
    "                X=words[i]\n",
    "                y=torch.tensor(targets[i]).double()\n",
    "                y=torch.tensor(y,requires_grad=True)\n",
    "\n",
    "                pred=self.forward(X)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fn(pred, y)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()  \n",
    "\n",
    "                error+=sum(np.round(y.data.numpy() - pred.data.numpy()))/4\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Testing network @ epoch {epoch}\")\n",
    "                print(\"Error:\",error/len(words))\n",
    "                    \n",
    "    def forward(self,input,n=3):\n",
    "        #input = torch.randn(self.batch_size, 3, 1) #fake input for debug\n",
    "        input=torch.tensor(np.array(input[np.newaxis,np.newaxis,:])).reshape(1,1,4).float()\n",
    "        hn=self.h0\n",
    "        output=input\n",
    "        for i in range(n):\n",
    "            output, hn = self.rnn(output, hn)\n",
    "       \n",
    "        assert len(output)<=self.batch_size,\"Invalid batchsize output\" #validate output\n",
    "        output=output.detach().numpy() #convert to numpy and reshape\n",
    "        output=np.sum(output,axis=1)[:,np.newaxis] #get the sum of the values\n",
    "        return torch.tensor(output[0][0])\n",
    "    def getAction(self,word):\n",
    "        vec=np.array(self.words.get(word,np.array([0,0,0,0]))) #give default\n",
    "        word_vec = torch.round(self.forward(vec)).detach().numpy() #gather vector\n",
    "        w=list(self.words.keys())[0]\n",
    "        for key in self.words:\n",
    "            if self.words[key].all()==word_vec.all(): #loop through\n",
    "                w=key\n",
    "        return w\n",
    "    def get_accuracy(self,words,labels): #get accuracy\n",
    "        acc=0\n",
    "        for word,label in zip(words,labels): #zip them together\n",
    "            act=self.getAction(word) #get action from this word\n",
    "            if act==label:\n",
    "                acc+=1\n",
    "        return acc/len(word) #return \n",
    "\n",
    "        \n",
    "net=R_Network(words_train)\n",
    "net.forward(x[0])\n",
    "net.train(words_train[0:10],x[0:10],epochs=100)\n",
    "print(\"Accuracy\",net.get_accuracy(words_train[0:10],x[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -7.0263,   6.8147,  -6.1705,   2.0332,  10.8733,  -5.4604,  -0.3081,\n",
      "         -7.8788,  -1.7301,  -2.6504,  -2.7632,  -1.7217,  -2.4411,   2.0993,\n",
      "          5.5797,  -4.9541,   1.3451,   1.4812,  -6.9932,  -2.6261,  -2.1535,\n",
      "         -1.3742,   0.8028,  -5.3220,  -3.4034,  -5.4714,  -0.6734,   3.8546,\n",
      "         -9.9450,   8.1569,  -1.9101,   5.6007,   3.3800,   0.7382,   1.2677,\n",
      "          5.9711,   2.3239,   0.0953,  -0.0145,  -3.4464,   5.4084,   0.9519,\n",
      "          0.2902,   5.5677,  -8.0937,   9.1140,   3.3787,   5.6431,  -2.8380,\n",
      "         14.4866,  -2.9227,   2.5693,  -5.4530,  -0.4428,   5.0368,   1.5918,\n",
      "         -8.9663,  -4.1025,  -9.1527,   1.2496,   4.3289,  -7.9883,  -1.9852,\n",
      "          7.3676,  -3.7074,   1.9989,  -4.5460,   0.2901,  -1.7207,  -0.5794,\n",
      "          1.0810,   0.5027,  -3.2028,  -6.0610,  -2.9303,   1.6782,   5.6528,\n",
      "          8.7015,  -2.8451,  -9.1710,   5.0370,   2.6042, -11.4110,  -1.7010,\n",
      "          1.1096,  -5.8426,   2.4253,  -3.3925,  -2.5130,   1.0066,  -3.9260,\n",
      "          7.9476,   2.4039,  -2.7117,   4.2286,  -0.2206,  -3.1213,   7.7995,\n",
      "         11.3332,  -2.6892], grad_fn=<SelectBackward>)\n",
      "Training...\n",
      "Accuracy 0.02\n",
      "Testing network @ epoch 0\n",
      "Error: 13.045906792097995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dexter Shepherd\\AppData\\Local\\Temp\\ipykernel_6556\\3677693456.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y=torch.tensor(y,requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.02\n",
      "Testing network @ epoch 200\n",
      "Error: 0.5731528520530924\n",
      "Accuracy 0.02\n",
      "Testing network @ epoch 400\n",
      "Error: 0.5472699021241044\n",
      "Accuracy 0.02\n",
      "Testing network @ epoch 600\n",
      "Error: 0.5388533678622006\n",
      "Accuracy 0.02\n",
      "Testing network @ epoch 800\n",
      "Error: 0.5343723129708985\n",
      "Accuracy 0.03\n",
      "[ 0  0  0 12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, 12])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FNN:\n",
    "    def __init__(self,words,inp=4,hid=10,out=4): #feed forward neural network\n",
    "        self.inp = torch.nn.Parameter(torch.randn(inp, hid)) #output hidden\n",
    "        self.h0 = torch.nn.Parameter(torch.randn(hid, hid)) #output hidden\n",
    "        self.h1 = torch.nn.Parameter(torch.randn(hid, out)) #output hidden\n",
    "        self.b1 = torch.nn.Parameter(torch.randn(1, hid)) #output hidden\n",
    "        self.b2 = torch.nn.Parameter(torch.randn(1, out)) #output hidden\n",
    "        self.words={}\n",
    "        vecs=convert_to_vector(words)\n",
    "        for i,word in enumerate(words): #create dict\n",
    "            self.words[word]=vecs[i]\n",
    "    def forward(self,item): #pass through network\n",
    "        item=item[:,np.newaxis]\n",
    "        x=torch.tensor(item).float()\n",
    "        x=(torch.mm(x.T, self.inp)) #torch.sigmoid\n",
    "        x=torch.sigmoid(torch.mm(x, self.h0))\n",
    "        x=(torch.mm(x, self.h1))\n",
    "        return x[0]\n",
    "    def train(self,words,targets,epochs=400):\n",
    "        print(\"Training...\")\n",
    "        #get each word in index\n",
    "        \n",
    "        #get words in relation of neigbours\n",
    "        #train word to next word\n",
    "        BestInp=None\n",
    "        Besth0=None\n",
    "        Besth1=None\n",
    "        BestAcc=0\n",
    "        optimizer = torch.optim.SGD((self.inp,self.h0,self.h1), lr=0.30) #, momentum=0.9\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        for epoch in range(epochs):\n",
    "            error=0\n",
    "            correct=0\n",
    "            for i in range(len(words)):\n",
    "                X=self.words.get(words[i],np.array([0,0,0,0]))/80\n",
    "                y=torch.tensor(targets[i]).double()\n",
    "                y=torch.tensor(y,requires_grad=True)\n",
    "               \n",
    "                pred=self.forward(X)\n",
    "                #X=torch.tensor(torch.tensor(X).double(),requires_grad=True)\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fn(pred, y)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()  \n",
    "                if np.argmax(pred.data.numpy())==i:\n",
    "                    correct+=1\n",
    "                error+=sum(abs(y.data.numpy() - pred.data.numpy()))/4\n",
    "            if correct/len(words) > BestAcc:\n",
    "                BestAcc=correct\n",
    "                BestInp=copy.deepcopy(self.inp)\n",
    "                Besth0=copy.deepcopy(self.h0)\n",
    "                Besth1=copy.deepcopy(self.h1)\n",
    "            if epoch %200 == 0:\n",
    "                #print(loss,torch.round(pred),y)\n",
    "                #print(sum((y.data.numpy() - pred.data.numpy()))/4)\n",
    "                print(\"Accuracy\",correct/len(words))\n",
    "                print(f\"Testing network @ epoch {epoch}\")\n",
    "                print(\"Error:\",error/len(words))\n",
    "        self.inp=BestInp\n",
    "        self.h0=Besth0\n",
    "        self.h1=Besth1\n",
    "    def getWordThing(self,word):\n",
    "        a=np.array([0,0,0,0])\n",
    "        print(self.words.get(word,a))\n",
    "        return self.words.get(word,a)\n",
    "    def get_action(self,word):\n",
    "        vec=self.words.get(word,np.array([0,0,0,0]))/80 #give default\n",
    "        word_vec = self.forward(vec).data.numpy() #gather vector\n",
    "        ind=np.argmax(word_vec)\n",
    "        #print(word,word_vec,ind)\n",
    "        return list(self.words.keys())[ind]\n",
    "    def getWord(self,word_vec):\n",
    "        w=\"None\"\n",
    "        for key in self.words:\n",
    "            #print(self.words[key],word_vec)\n",
    "            if np.array_equal(self.words[key],word_vec): #loop through\n",
    "                w=key\n",
    "        return w\n",
    "    def get_accuracy(self,words,labels): #get accuracy\n",
    "        acc=0\n",
    "        for word,label in zip(words,labels): #zip them together\n",
    "            act=self.get_action(word) #get action from this word\n",
    "            #print(act)\n",
    "            if act==word:\n",
    "                acc+=1\n",
    "        return acc/len(words) #return accuracy\n",
    "\n",
    "\n",
    "words_train=get_word_list(train_words)\n",
    "labels=get_label_as_data(words_train[0:100])\n",
    "netF=FNN(words_train,hid=len(labels[0])//2,out=len(labels[0]))\n",
    "print(netF.forward(x[0]))#\n",
    "#print(words_train[0:4],labels[0:4])\n",
    "netF.train(words_train[0:100],labels[0:100],epochs=10000)  \n",
    "print(\"End Accuracy\",netF.get_accuracy(words_train[0:100],labels[0:100]))\n",
    "netF.getWordThing(words_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_defineLayers:\n",
    "    def __init__(self, num_input, layers, num_output):\n",
    "        assert type(layers)==type([]), \"Error with layers, give array of the number of layers\"\n",
    "        self.num_input = num_input  #set input number\n",
    "        self.num_output = num_output #set ooutput number\n",
    "        self.hidden=[]\n",
    "        last=num_input\n",
    "        self.num_genes=0\n",
    "        for layer in layers:\n",
    "            self.hidden.append(layer)\n",
    "            self.num_genes+=(last * layer)\n",
    "            last=layer\n",
    "        self.num_genes +=(self.hidden[-1]*num_output)+num_output\n",
    "        self.weights = None\n",
    "        self.hidden_weights=None\n",
    "        self.bias = None\n",
    "    def set_genes(self, gene):\n",
    "        weight_idxs = self.num_input * self.hidden[0] #size of weights to hidden\n",
    "        current=weight_idxs\n",
    "        weights_idxs=[current] #start with end of last\n",
    "        for i in range(len(self.hidden)-1):\n",
    "            current+=self.hidden[i]*self.hidden[i+1] #calculate next idx for each layer\n",
    "            weights_idxs.append(current)\n",
    "        bias_idxs=None\n",
    "        weights_idxs.append(self.hidden[-1] * self.num_output + weights_idxs[-1]) #add last layer heading to output\n",
    "        bias_idxs = weights_idxs[-1]+ self.num_output #sizes of biases\n",
    "        w = gene[0 : weight_idxs].reshape(self.hidden[0], self.num_input)   #merge genes\n",
    "        ws=[]\n",
    "        for i in range(len(self.hidden)-2):\n",
    "            ws.append(gene[weights_idxs[i] : weights_idxs[i+1]].reshape(self.hidden[i+1], self.hidden[i]))\n",
    "        ws.append(gene[weights_idxs[-2] : weights_idxs[-1]].reshape(self.num_output, self.hidden[-1]))\n",
    "        b = gene[weights_idxs[-1]: bias_idxs].reshape(self.num_output,) #merge genes\n",
    "\n",
    "        self.weights = torch.from_numpy(w) #assign weights\n",
    "        self.hidden_weights=[]\n",
    "        for w in ws:\n",
    "            self.hidden_weights.append(torch.from_numpy(w))\n",
    "        self.bias = torch.from_numpy(b) #assign biases\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x[:,np.newaxis]\n",
    "        x = torch.from_numpy(x).double()\n",
    "        x = torch.mm(x.T, self.weights.T) #first layer\n",
    "        for i in range(len(self.hidden_weights)-1):\n",
    "            x = torch.mm(x,self.hidden_weights[i].T) #second layer\n",
    "        return torch.mm(x,self.hidden_weights[-1].T) + self.bias #third layer\n",
    "        \n",
    "    def get_action(self, x):\n",
    "        arr=torch.round(self.forward(x)[0])\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0 accuracy 0\n",
      "Generation 1 accuracy 0\n",
      "Generation 2 accuracy 0\n",
      "Generation 3 accuracy 0\n",
      "Generation 4 accuracy 0\n",
      "Generation 5 accuracy 0\n",
      "Generation 6 accuracy 0\n",
      "Generation 7 accuracy 0\n",
      "Generation 8 accuracy 0\n",
      "Generation 9 accuracy 0\n",
      "Generation 10 accuracy 0\n",
      "Generation 11 accuracy 0\n",
      "Generation 12 accuracy 0\n",
      "Generation 13 accuracy 0\n",
      "Generation 14 accuracy 0\n",
      "Generation 15 accuracy 0\n",
      "Generation 16 accuracy 0\n",
      "Generation 17 accuracy 0\n",
      "Generation 18 accuracy 0\n",
      "Generation 19 accuracy 0\n",
      "Generation 20 accuracy 0\n",
      "Generation 21 accuracy 0\n",
      "Generation 22 accuracy 0\n",
      "Generation 23 accuracy 0\n",
      "Generation 24 accuracy 0\n",
      "Generation 25 accuracy 0\n",
      "Generation 26 accuracy 0\n",
      "Generation 27 accuracy 0\n",
      "Generation 28 accuracy 0\n",
      "Generation 29 accuracy 0\n",
      "Generation 30 accuracy 0\n",
      "Generation 31 accuracy 0\n",
      "Generation 32 accuracy 0\n",
      "Generation 33 accuracy 0\n",
      "Generation 34 accuracy 0\n",
      "Generation 35 accuracy 0\n",
      "Generation 36 accuracy 0\n",
      "Generation 37 accuracy 0\n",
      "Generation 38 accuracy 0\n",
      "Generation 39 accuracy 0\n",
      "Generation 40 accuracy 0\n",
      "Generation 41 accuracy 0\n",
      "Generation 42 accuracy 0\n",
      "Generation 43 accuracy 0\n",
      "Generation 44 accuracy 0\n",
      "Generation 45 accuracy 0\n",
      "Generation 46 accuracy 0\n",
      "Generation 47 accuracy 0\n",
      "Generation 48 accuracy 0\n",
      "Generation 49 accuracy 0\n",
      "Generation 50 accuracy 0\n",
      "Generation 51 accuracy 0\n",
      "Generation 52 accuracy 0\n",
      "Generation 53 accuracy 0\n",
      "Generation 54 accuracy 0\n",
      "Generation 55 accuracy 0\n",
      "Generation 56 accuracy 0\n",
      "Generation 57 accuracy 0\n",
      "Generation 58 accuracy 0\n",
      "Generation 59 accuracy 0\n",
      "Generation 60 accuracy 0\n",
      "Generation 61 accuracy 0\n",
      "Generation 62 accuracy 0\n",
      "Generation 63 accuracy 0\n",
      "Generation 64 accuracy 0\n",
      "Generation 65 accuracy 0\n",
      "Generation 66 accuracy 0\n",
      "Generation 67 accuracy 0\n",
      "Generation 68 accuracy 0\n",
      "Generation 69 accuracy 0\n",
      "Generation 70 accuracy 0\n",
      "Generation 71 accuracy 0\n",
      "Generation 72 accuracy 0\n",
      "Generation 73 accuracy 0\n",
      "Generation 74 accuracy 0\n",
      "Generation 75 accuracy 0\n",
      "Generation 76 accuracy 0\n",
      "Generation 77 accuracy 0\n",
      "Generation 78 accuracy 0\n",
      "Generation 79 accuracy 0\n",
      "Generation 80 accuracy 0\n",
      "Generation 81 accuracy 0\n",
      "Generation 82 accuracy 0\n",
      "Generation 83 accuracy 0\n",
      "Generation 84 accuracy 0\n",
      "Generation 85 accuracy 0\n",
      "Generation 86 accuracy 0\n",
      "Generation 87 accuracy 0\n",
      "Generation 88 accuracy 0\n",
      "Generation 89 accuracy 0\n",
      "Generation 90 accuracy 0\n",
      "Generation 91 accuracy 0\n",
      "Generation 92 accuracy 0\n",
      "Generation 93 accuracy 0\n",
      "Generation 94 accuracy 0\n",
      "Generation 95 accuracy 0\n",
      "Generation 96 accuracy 0\n",
      "Generation 97 accuracy 0\n",
      "Generation 98 accuracy 0\n",
      "Generation 99 accuracy 0\n",
      "Generation 100 accuracy 0\n",
      "Generation 101 accuracy 0\n",
      "Generation 102 accuracy 0\n",
      "Generation 103 accuracy 0\n",
      "Generation 104 accuracy 0\n",
      "Generation 105 accuracy 0\n",
      "Generation 106 accuracy 0\n",
      "Generation 107 accuracy 0\n",
      "Generation 108 accuracy 0\n",
      "Generation 109 accuracy 0\n",
      "Generation 110 accuracy 0\n",
      "Generation 111 accuracy 0\n",
      "Generation 112 accuracy 0\n",
      "Generation 113 accuracy 0\n",
      "Generation 114 accuracy 0\n",
      "Generation 115 accuracy 0\n",
      "Generation 116 accuracy 0\n",
      "Generation 117 accuracy 0\n",
      "Generation 118 accuracy 0\n",
      "Generation 119 accuracy 0\n",
      "Generation 120 accuracy 0\n",
      "Generation 121 accuracy 0\n",
      "Generation 122 accuracy 0\n",
      "Generation 123 accuracy 0\n",
      "Generation 124 accuracy 0\n",
      "Generation 125 accuracy 0\n",
      "Generation 126 accuracy 0\n",
      "Generation 127 accuracy 0\n",
      "Generation 128 accuracy 0\n",
      "Generation 129 accuracy 0\n",
      "Generation 130 accuracy 0\n",
      "Generation 131 accuracy 0\n",
      "Generation 132 accuracy 0\n",
      "Generation 133 accuracy 0\n",
      "Generation 134 accuracy 0\n",
      "Generation 135 accuracy 0\n",
      "Generation 136 accuracy 0\n",
      "Generation 137 accuracy 0\n",
      "Generation 138 accuracy 0\n",
      "Generation 139 accuracy 0\n",
      "Generation 140 accuracy 0\n",
      "Generation 141 accuracy 0\n",
      "Generation 142 accuracy 0\n",
      "Generation 143 accuracy 0\n",
      "Generation 144 accuracy 0\n",
      "Generation 145 accuracy 0\n",
      "Generation 146 accuracy 0\n",
      "Generation 147 accuracy 0\n",
      "Generation 148 accuracy 0\n",
      "Generation 149 accuracy 0\n",
      "Generation 150 accuracy 0\n",
      "Generation 151 accuracy 0\n",
      "Generation 152 accuracy 0\n",
      "Generation 153 accuracy 0\n",
      "Generation 154 accuracy 0\n",
      "Generation 155 accuracy 0\n",
      "Generation 156 accuracy 0\n",
      "Generation 157 accuracy 0\n",
      "Generation 158 accuracy 0\n",
      "Generation 159 accuracy 0\n",
      "Generation 160 accuracy 0\n",
      "Generation 161 accuracy 0\n",
      "Generation 162 accuracy 0\n",
      "Generation 163 accuracy 0\n",
      "Generation 164 accuracy 0\n",
      "Generation 165 accuracy 0\n",
      "Generation 166 accuracy 0\n",
      "Generation 167 accuracy 0\n",
      "Generation 168 accuracy 0\n",
      "Generation 169 accuracy 0\n",
      "Generation 170 accuracy 0\n",
      "Generation 171 accuracy 0\n",
      "Generation 172 accuracy 0\n",
      "Generation 173 accuracy 0\n",
      "Generation 174 accuracy 0\n",
      "Generation 175 accuracy 0\n",
      "Generation 176 accuracy 0\n",
      "Generation 177 accuracy 0\n",
      "Generation 178 accuracy 0\n",
      "Generation 179 accuracy 0\n",
      "Generation 180 accuracy 0\n",
      "Generation 181 accuracy 0\n",
      "Generation 182 accuracy 0\n",
      "Generation 183 accuracy 0\n",
      "Generation 184 accuracy 0\n",
      "Generation 185 accuracy 0\n",
      "Generation 186 accuracy 0\n",
      "Generation 187 accuracy 0\n",
      "Generation 188 accuracy 0\n",
      "Generation 189 accuracy 0\n",
      "Generation 190 accuracy 0\n",
      "Generation 191 accuracy 0\n",
      "Generation 192 accuracy 0\n",
      "Generation 193 accuracy 0\n",
      "Generation 194 accuracy 0\n",
      "Generation 195 accuracy 0\n",
      "Generation 196 accuracy 0\n",
      "Generation 197 accuracy 0\n",
      "Generation 198 accuracy 0\n",
      "Generation 199 accuracy 0\n",
      "Generation 200 accuracy 0\n",
      "Generation 201 accuracy 0\n",
      "Generation 202 accuracy 0\n",
      "Generation 203 accuracy 0\n",
      "Generation 204 accuracy 0\n",
      "Generation 205 accuracy 0\n",
      "Generation 206 accuracy 0\n",
      "Generation 207 accuracy 0\n",
      "Generation 208 accuracy 0\n",
      "Generation 209 accuracy 0\n",
      "Generation 210 accuracy 0\n",
      "Generation 211 accuracy 0\n",
      "Generation 212 accuracy 0\n",
      "Generation 213 accuracy 0\n",
      "Generation 214 accuracy 0\n",
      "Generation 215 accuracy 0\n",
      "Generation 216 accuracy 0\n",
      "Generation 217 accuracy 0\n",
      "Generation 218 accuracy 0\n",
      "Generation 219 accuracy 0\n",
      "Generation 220 accuracy 0\n",
      "Generation 221 accuracy 0\n",
      "Generation 222 accuracy 0\n",
      "Generation 223 accuracy 0\n",
      "Generation 224 accuracy 0\n",
      "Generation 225 accuracy 0\n",
      "Generation 226 accuracy 0\n",
      "Generation 227 accuracy 0\n",
      "Generation 228 accuracy 0\n",
      "Generation 229 accuracy 0\n",
      "Generation 230 accuracy 0\n",
      "Generation 231 accuracy 0\n",
      "Generation 232 accuracy 0\n",
      "Generation 233 accuracy 0\n",
      "Generation 234 accuracy 0\n",
      "Generation 235 accuracy 0\n",
      "Generation 236 accuracy 0\n",
      "Generation 237 accuracy 0\n",
      "Generation 238 accuracy 0\n",
      "Generation 239 accuracy 0\n",
      "Generation 240 accuracy 0\n",
      "Generation 241 accuracy 0\n",
      "Generation 242 accuracy 0\n",
      "Generation 243 accuracy 0\n",
      "Generation 244 accuracy 0\n",
      "Generation 245 accuracy 0\n",
      "Generation 246 accuracy 0\n",
      "Generation 247 accuracy 0\n",
      "Generation 248 accuracy 0\n",
      "Generation 249 accuracy 0\n",
      "Generation 250 accuracy 0\n",
      "Generation 251 accuracy 0\n",
      "Generation 252 accuracy 0\n",
      "Generation 253 accuracy 0\n",
      "Generation 254 accuracy 0\n",
      "Generation 255 accuracy 0\n",
      "Generation 256 accuracy 0\n",
      "Generation 257 accuracy 0\n",
      "Generation 258 accuracy 0\n",
      "Generation 259 accuracy 0\n",
      "Generation 260 accuracy 0\n",
      "Generation 261 accuracy 0\n",
      "Generation 262 accuracy 0\n",
      "Generation 263 accuracy 0\n",
      "Generation 264 accuracy 0\n",
      "Generation 265 accuracy 0\n",
      "Generation 266 accuracy 0\n",
      "Generation 267 accuracy 0\n",
      "Generation 268 accuracy 0\n",
      "Generation 269 accuracy 0\n",
      "Generation 270 accuracy 0\n",
      "Generation 271 accuracy 0\n",
      "Generation 272 accuracy 0\n",
      "Generation 273 accuracy 0\n",
      "Generation 274 accuracy 0\n",
      "Generation 275 accuracy 0\n",
      "Generation 276 accuracy 0\n",
      "Generation 277 accuracy 0\n",
      "Generation 278 accuracy 0\n",
      "Generation 279 accuracy 0\n",
      "Generation 280 accuracy 0\n",
      "Generation 281 accuracy 0\n",
      "Generation 282 accuracy 0\n",
      "Generation 283 accuracy 0\n",
      "Generation 284 accuracy 0\n",
      "Generation 285 accuracy 0\n",
      "Generation 286 accuracy 0\n",
      "Generation 287 accuracy 0\n",
      "Generation 288 accuracy 0\n",
      "Generation 289 accuracy 0\n",
      "Generation 290 accuracy 0\n",
      "Generation 291 accuracy 0\n",
      "Generation 292 accuracy 0\n",
      "Generation 293 accuracy 0\n",
      "Generation 294 accuracy 0\n",
      "Generation 295 accuracy 0\n",
      "Generation 296 accuracy 0\n",
      "Generation 297 accuracy 0\n",
      "Generation 298 accuracy 0\n",
      "Generation 299 accuracy 0\n",
      "Generation 300 accuracy 0\n",
      "Generation 301 accuracy 0\n",
      "Generation 302 accuracy 0\n",
      "Generation 303 accuracy 0\n",
      "Generation 304 accuracy 0\n",
      "Generation 305 accuracy 0\n",
      "Generation 306 accuracy 0\n",
      "Generation 307 accuracy 0\n",
      "Generation 308 accuracy 0\n",
      "Generation 309 accuracy 0\n",
      "Generation 310 accuracy 0\n",
      "Generation 311 accuracy 0\n",
      "Generation 312 accuracy 0\n",
      "Generation 313 accuracy 0\n",
      "Generation 314 accuracy 0\n",
      "Generation 315 accuracy 0\n",
      "Generation 316 accuracy 0\n",
      "Generation 317 accuracy 0\n",
      "Generation 318 accuracy 0\n",
      "Generation 319 accuracy 0\n",
      "Generation 320 accuracy 0\n",
      "Generation 321 accuracy 0\n",
      "Generation 322 accuracy 0\n",
      "Generation 323 accuracy 0\n",
      "Generation 324 accuracy 0\n",
      "Generation 325 accuracy 0\n",
      "Generation 326 accuracy 0\n",
      "Generation 327 accuracy 0\n",
      "Generation 328 accuracy 0\n",
      "Generation 329 accuracy 0\n",
      "Generation 330 accuracy 0\n",
      "Generation 331 accuracy 0\n",
      "Generation 332 accuracy 0\n",
      "Generation 333 accuracy 0\n",
      "Generation 334 accuracy 0\n",
      "Generation 335 accuracy 0\n",
      "Generation 336 accuracy 0\n",
      "Generation 337 accuracy 0\n",
      "Generation 338 accuracy 0\n",
      "Generation 339 accuracy 0\n",
      "Generation 340 accuracy 0\n",
      "Generation 341 accuracy 0\n",
      "Generation 342 accuracy 0\n",
      "Generation 343 accuracy 0\n",
      "Generation 344 accuracy 0\n",
      "Generation 345 accuracy 0\n",
      "Generation 346 accuracy 0\n",
      "Generation 347 accuracy 0\n",
      "Generation 348 accuracy 0\n",
      "Generation 349 accuracy 0\n",
      "Generation 350 accuracy 0\n",
      "Generation 351 accuracy 0\n",
      "Generation 352 accuracy 0\n",
      "Generation 353 accuracy 0\n",
      "Generation 354 accuracy 0\n",
      "Generation 355 accuracy 0\n",
      "Generation 356 accuracy 0\n",
      "Generation 357 accuracy 0\n",
      "Generation 358 accuracy 0\n",
      "Generation 359 accuracy 0\n",
      "Generation 360 accuracy 0\n",
      "Generation 361 accuracy 0\n",
      "Generation 362 accuracy 0\n",
      "Generation 363 accuracy 0\n",
      "Generation 364 accuracy 0\n",
      "Generation 365 accuracy 0\n",
      "Generation 366 accuracy 0\n",
      "Generation 367 accuracy 0\n",
      "Generation 368 accuracy 0\n",
      "Generation 369 accuracy 0\n",
      "Generation 370 accuracy 0\n",
      "Generation 371 accuracy 0\n",
      "Generation 372 accuracy 0\n",
      "Generation 373 accuracy 0\n",
      "Generation 374 accuracy 0\n",
      "Generation 375 accuracy 0\n",
      "Generation 376 accuracy 0\n",
      "Generation 377 accuracy 0\n",
      "Generation 378 accuracy 0\n",
      "Generation 379 accuracy 0\n",
      "Generation 380 accuracy 0\n",
      "Generation 381 accuracy 0\n",
      "Generation 382 accuracy 0\n",
      "Generation 383 accuracy 0\n",
      "Generation 384 accuracy 0\n",
      "Generation 385 accuracy 0\n",
      "Generation 386 accuracy 0\n",
      "Generation 387 accuracy 0\n",
      "Generation 388 accuracy 0\n",
      "Generation 389 accuracy 0\n",
      "Generation 390 accuracy 0\n",
      "Generation 391 accuracy 0\n",
      "Generation 392 accuracy 0\n",
      "Generation 393 accuracy 0\n",
      "Generation 394 accuracy 0\n",
      "Generation 395 accuracy 0\n",
      "Generation 396 accuracy 0\n",
      "Generation 397 accuracy 0\n",
      "Generation 398 accuracy 0\n",
      "Generation 399 accuracy 0\n",
      "Generation 400 accuracy 0\n",
      "Generation 401 accuracy 0\n",
      "Generation 402 accuracy 0\n",
      "Generation 403 accuracy 0\n",
      "Generation 404 accuracy 0\n",
      "Generation 405 accuracy 0\n",
      "Generation 406 accuracy 0\n",
      "Generation 407 accuracy 0\n",
      "Generation 408 accuracy 0\n",
      "Generation 409 accuracy 0\n",
      "Generation 410 accuracy 0\n",
      "Generation 411 accuracy 0\n",
      "Generation 412 accuracy 0\n",
      "Generation 413 accuracy 0\n",
      "Generation 414 accuracy 0\n",
      "Generation 415 accuracy 0\n",
      "Generation 416 accuracy 0\n",
      "Generation 417 accuracy 0\n",
      "Generation 418 accuracy 0\n",
      "Generation 419 accuracy 0\n",
      "Generation 420 accuracy 0\n",
      "Generation 421 accuracy 0\n",
      "Generation 422 accuracy 0\n",
      "Generation 423 accuracy 0\n",
      "Generation 424 accuracy 0\n",
      "Generation 425 accuracy 0\n",
      "Generation 426 accuracy 0\n",
      "Generation 427 accuracy 0\n",
      "Generation 428 accuracy 0\n",
      "Generation 429 accuracy 0\n",
      "Generation 430 accuracy 0\n",
      "Generation 431 accuracy 0\n",
      "Generation 432 accuracy 0\n",
      "Generation 433 accuracy 0\n",
      "Generation 434 accuracy 0\n",
      "Generation 435 accuracy 0\n",
      "Generation 436 accuracy 0\n",
      "Generation 437 accuracy 0\n",
      "Generation 438 accuracy 0\n",
      "Generation 439 accuracy 0\n",
      "Generation 440 accuracy 0\n",
      "Generation 441 accuracy 0\n",
      "Generation 442 accuracy 0\n",
      "Generation 443 accuracy 0\n",
      "Generation 444 accuracy 0\n",
      "Generation 445 accuracy 0\n",
      "Generation 446 accuracy 0\n",
      "Generation 447 accuracy 0\n",
      "Generation 448 accuracy 0\n",
      "Generation 449 accuracy 0\n",
      "Generation 450 accuracy 0\n",
      "Generation 451 accuracy 0\n",
      "Generation 452 accuracy 0\n",
      "Generation 453 accuracy 0\n",
      "Generation 454 accuracy 0\n",
      "Generation 455 accuracy 0\n",
      "Generation 456 accuracy 0\n",
      "Generation 457 accuracy 0\n",
      "Generation 458 accuracy 0\n",
      "Generation 459 accuracy 0\n",
      "Generation 460 accuracy 0\n",
      "Generation 461 accuracy 0\n",
      "Generation 462 accuracy 0\n",
      "Generation 463 accuracy 0\n",
      "Generation 464 accuracy 0\n",
      "Generation 465 accuracy 0\n",
      "Generation 466 accuracy 0\n",
      "Generation 467 accuracy 0\n",
      "Generation 468 accuracy 0\n",
      "Generation 469 accuracy 0\n",
      "Generation 470 accuracy 0\n",
      "Generation 471 accuracy 0\n",
      "Generation 472 accuracy 0\n",
      "Generation 473 accuracy 0\n",
      "Generation 474 accuracy 0\n",
      "Generation 475 accuracy 0\n",
      "Generation 476 accuracy 0\n",
      "Generation 477 accuracy 0\n",
      "Generation 478 accuracy 0\n",
      "Generation 479 accuracy 0\n",
      "Generation 480 accuracy 0\n",
      "Generation 481 accuracy 0\n",
      "Generation 482 accuracy 0\n",
      "Generation 483 accuracy 0\n",
      "Generation 484 accuracy 0\n",
      "Generation 485 accuracy 0\n",
      "Generation 486 accuracy 0\n",
      "Generation 487 accuracy 0\n",
      "Generation 488 accuracy 0\n",
      "Generation 489 accuracy 0\n",
      "Generation 490 accuracy 0\n",
      "Generation 491 accuracy 0\n",
      "Generation 492 accuracy 0\n",
      "Generation 493 accuracy 0\n",
      "Generation 494 accuracy 0\n",
      "Generation 495 accuracy 0\n",
      "Generation 496 accuracy 0\n",
      "Generation 497 accuracy 0\n",
      "Generation 498 accuracy 0\n",
      "Generation 499 accuracy 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "words={}\n",
    "vecs=convert_to_vector(words_train)\n",
    "for i,word in enumerate(words_train): #create dict\n",
    "    words[word]=vecs[i]\n",
    "\n",
    "def getWord(word_vec):\n",
    "        w=\"None\"\n",
    "        for key in words:\n",
    "            #print(self.words[key],type(word_vec))\n",
    "            if np.array_equal(words[key],word_vec): #loop through\n",
    "                w=key\n",
    "        return w\n",
    "def runFitness(agent,words,labels):\n",
    "    acc=0\n",
    "    for word,label in zip(words,labels): #zip them together\n",
    "        act=agent.get_action(word) #get action from this word\n",
    "        if act==getWord(label):\n",
    "            acc+=1\n",
    "    return acc/len(words) #return accuracy\n",
    "\n",
    "def mutation(gene, mean=0, std=0.5,size=100):\n",
    "    assert size<len(gene)\n",
    "    n=random.randint(0,len(gene)-size-1)\n",
    "    array=np.random.normal(mean,std,size=size)\n",
    "    gene = gene[n:n+size] + array #mutate the gene via normal \n",
    "    # constraint\n",
    "    gene[gene >4] = 4\n",
    "    gene[gene < -4] = -4\n",
    "    return gene\n",
    "\n",
    "whegBot=Agent_defineLayers(4,[10,10],4) #define the agent\n",
    "\n",
    "pop_size=150\n",
    "gene_pop=[]\n",
    "for i in range(pop_size): #vary from 10 to 20 depending on purpose of robot\n",
    "    gene=np.random.normal(0, 0.5, (whegBot.num_genes))\n",
    "    gene_pop.append(gene)#create\n",
    "\n",
    "acc=[0]\n",
    "for gen in range(500):\n",
    "    ind_1 = random.randint(0,len(gene_pop)-1)\n",
    "    ind_2=0\n",
    "    if ind_1>0: ind_2 = ind_1-1\n",
    "    else: ind_2= ind_1+1\n",
    "    #get both genes\n",
    "    gene1=gene_pop[ind_1]\n",
    "    gene2=gene_pop[ind_2]\n",
    "    #run trials\n",
    "    whegBot.set_genes(gene1)\n",
    "    fit1=runFitness(whegBot,x[0:10],x[0:10])\n",
    "    whegBot.set_genes(gene2)\n",
    "    fit2=runFitness(whegBot,x[0:10],x[0:10])\n",
    "    #selection\n",
    "    if fit1>fit2:\n",
    "        gene_pop[ind_2]=np.copy(mutation(gene1))\n",
    "    elif fit2>fit1:\n",
    "        gene_pop[ind_1]=np.copy(mutation(gene1))\n",
    "    acc.append(max(max(acc),max([fit1,fit2])))\n",
    "    print(\"Generation\",gen,\"accuracy\",max(acc))\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An attempt to make a feed forward neural network was placed in as there were issues with backpropagating the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4dd024378a2eab6b61421bb2db15dae0e0d8b99b3e2b4f86d231a4685d0f22e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
