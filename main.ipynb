{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical assignment\n",
    "\n",
    "The Microsoft Research Sentence Completion Challenge (Zweig and Burges, 2011) requires a system to\n",
    "be able to predict which is the most likely word (from a set of 5 possibilities) to complete a sentence. In\n",
    "the labs you have evaluated using unigram and bigram models. In this assignment you are expected to\n",
    "investigate at least 2 extensions or alternative approaches to making predictions. Your solution does\n",
    "not need to be novel. You might choose to investigate 2 of the following approaches or 1 of the following\n",
    "approaches and 1 of your own devising.\n",
    "\n",
    "•Tri-gram (or even quadrigram) models\n",
    "•Word similarity methods e.g., using Googlenews vectors or WordNet?\n",
    "•Combining n-gram methods with word similarity methods e.g., distributional smoothing?\n",
    "•Using a neural language model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os,random,math\n",
    "import sys\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "\n",
    "training_dir=\"D:/Documents/Computer Science/Year 3/Semester Two/ANLE/lab2resources/lab2resources\"+\"/sentence-completion/Holmes_Training_Data/\"\n",
    "#testing_dir=\"D:/Documents/Computer Science/Year 3/Semester Two/ANLE/lab2resources/lab2resources/sentence-completion/testing_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by calling in needed libraries and setting the directories of data. We next want to read the data and format it correctly.\n",
    "\n",
    "The files are split between training and testing data from the holmes training data set. We then gather every line in the file within two large string array for training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 522 files in the training directory: D:/Documents/Computer Science/Year 3/Semester Two/ANLE/lab2resources/lab2resources/sentence-completion/Holmes_Training_Data\n",
      "TCHMS10.TXT PETHL10.TXT\n",
      "['please take a look at the important information in this header', ' we encourage you to keep this file on your own disk, keeping an electronic path open for the next readers', ' do not remove this', '  **welcome to the world of free plain vanilla electronic texts** **etexts readable by both humans and by computers, since 1971** *these etexts prepared by hundreds of volunteers and donations* information on contacting project gutenberg to get etexts, and further information is included below', ' we need your donations', '  the arabian nights entertainments, by andrew lang may, 1994 [etext #128]  the project gutenberg etext of arabian nights, by andrew lang ****this file should be named arabn11', 'txt or arabn11', 'zip***** corrected editions of our etexts get a new number, arabn12', 'txt', ' versions based on separate sources get new letter, arabn11a'] [\"*project gutenberg's the picture of dorian gray by oscar wilde* *** etexts from the original internet information providers *** please take a look at the important information in this header\", ' we encourage you to keep this file on your own disk, keeping an electronic path open for the next readers', ' do not remove this', '  **welcome to the world of free plain vanilla electronic texts** **etexts readable by both humans and by computers, since 1971** *these etexts prepared by hundreds of volunteers and donations* information on contacting project gutenberg to get etexts, and further information is included below', ' we need your donations', \"  the picture of dorian gray by oscar wilde october, 1994 etext #174  *project gutenberg's the picture of dorian gray by oscar wilde* *****this file should be named dgray10\", 'txt or dgray10', 'zip***** corrected editions of our etexts get a new number, dgray11', 'txt', ' versions based on separate sources get new letter, dgray10a']\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR=os.path.dirname(training_dir) #this needs to be the parent directory for the training corpus\n",
    "def get_training_testing(training_dir=TRAINING_DIR,split=0.5):\n",
    "    filenames=os.listdir(training_dir)\n",
    "    n=len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
    "    #random.seed(53) #if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index=int(n*split)\n",
    "    return(filenames[:index],filenames[index:])\n",
    "\n",
    "def readWords(files):\n",
    "    training=[]\n",
    "    for afile in files: #look through each file\n",
    "                #print(\"Processing {}\".format(afile))\n",
    "                try:\n",
    "                    sent=\"\"\n",
    "                    with open(os.path.join(training_dir,afile)) as instream: #get each line and preprocess\n",
    "                        for line in instream:\n",
    "                            line=line.lower()\n",
    "                            line=line.rstrip()\n",
    "                            sent+=line+\" \" #gather each line in the array\n",
    "                    \n",
    "                except UnicodeDecodeError:\n",
    "                    print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "                except PermissionError:\n",
    "                    print(\"denied\")\n",
    "    sent=sent.replace(\"?\",\".\").replace(\"!\",\".\").replace(\"  \",\" \")\n",
    "    training=sent.split(\".\") #get array of sentences\n",
    "    try: training.remove(\" \")  #remove spaces\n",
    "    except ValueError: pass\n",
    "    try: training.remove(' m') #remove random characters\n",
    "    except ValueError: pass\n",
    "    return training\n",
    "train,test=get_training_testing() #get the data\n",
    "\n",
    "print(train[0],test[0])\n",
    "\n",
    "train_words=readWords(train) #get the train sentences\n",
    "test_words=readWords(test) #get the test sentences\n",
    "print(train_words[0:10],test_words[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply preprocessing techniques on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, 551])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def get_word_list(train):\n",
    "    dict={}\n",
    "    for sent in train:\n",
    "        for word in sent.split():\n",
    "            dict[word]=0\n",
    "    return list(dict.keys()) #list of unique words return\n",
    "def convert_to_vector(words):\n",
    "    oov_token = \"<OOV>\"\n",
    "    tokenizer = Tokenizer(num_words=len(words), oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(words)\n",
    "    #word_index = tokenizer.word_index\n",
    "    sequences = tokenizer.texts_to_sequences(words)\n",
    "    return list(pad_sequences(sequences, truncating='post', maxlen=4))\n",
    "def count_freq(train):\n",
    "    for sent in train:\n",
    "        pass\n",
    "words_train=get_word_list(train_words)\n",
    "x=convert_to_vector(words_train)\n",
    "\n",
    "def get_label_as_data(labels):\n",
    "    x=len(labels) #get the size of the array\n",
    "    arr=np.zeros((x)) #create an array of empty\n",
    "    lab=[]\n",
    "    for i in range(x):\n",
    "        a=np.copy(arr)\n",
    "        a[i]=1\n",
    "        lab.append(a)\n",
    "    return np.array(lab)\n",
    "INP_SIZE=len(x[0])\n",
    "\n",
    "x[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the first model\n",
    "\n",
    "A recurrent neural network is a good choice for this sort of task as the architecture makes it well suited to sequencing. This approach is taken with stochastic gradient descsent as stated in the research paper 'Character-Aware Neural Language Models'.\n",
    "\n",
    "The second approach used a feed-forward network where the vector as input and vector prediction as output. It encountered problems where the gradient optimization would grow infinitely smaller and eventually be an unrecognized value. Sigmoid was the only activation function that prevented this, however would force values between 0 and 1 resulting in incorrect predictions for a population of training data. \n",
    "\n",
    "The next model used labeled representation to represent the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dexter Shepherd\\AppData\\Local\\Temp\\ipykernel_21488\\646122950.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y=torch.tensor(y,requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing network @ epoch 0\n",
      "Error: 93.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dexter Shepherd\\AppData\\Local\\Temp\\ipykernel_21488\\646122950.py:66: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if act==label:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "class R_Network:\n",
    "    def __init__(self,wordList,inp=4,hid=4,out=4,batch_size=1):\n",
    "        self.rnn = torch.nn.RNN(inp,hid,out)\n",
    "        self.h0 = torch.nn.Parameter(torch.randn(inp, 1, hid)) #output hidden\n",
    "        self.batch_size=batch_size\n",
    "        self.words=[\"word\" for i in range(batch_size)] #initialize empty to prevent errors\n",
    "        self.wordList=wordList\n",
    "        self.inp=inp\n",
    "        self.words={}\n",
    "        vecs=convert_to_vector(wordList)\n",
    "        for i,word in enumerate(wordList): #create dict\n",
    "            self.words[word]=vecs[i]\n",
    "    def train(self,words,targets,epochs=500):\n",
    "        #get each word in index\n",
    "        words_train=get_word_list(words)\n",
    "        #vectorize words\n",
    "        words=convert_to_vector(words_train)\n",
    "        #get words in relation of neigbours\n",
    "        #train word to next word\n",
    "        optimizer = torch.optim.SGD((self.rnn.parameters()), lr=0.20) #, momentum=0.9\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        for epoch in range(epochs):\n",
    "            error=0\n",
    "            for i in range(len(words)):\n",
    "                X=words[i]\n",
    "                y=torch.tensor(targets[i]).double()\n",
    "                y=torch.tensor(y,requires_grad=True)\n",
    "\n",
    "                pred=self.forward(X)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fn(pred, y)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()  \n",
    "\n",
    "                error+=sum(np.round(y.data.numpy() - pred.data.numpy()))/4\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Testing network @ epoch {epoch}\")\n",
    "                print(\"Error:\",error/len(words))\n",
    "                    \n",
    "    def forward(self,input,n=3):\n",
    "        #input = torch.randn(self.batch_size, 3, 1) #fake input for debug\n",
    "        input=torch.tensor(np.array(input[np.newaxis,np.newaxis,:])).reshape(1,1,4).float()\n",
    "        hn=self.h0\n",
    "        output=input\n",
    "        for i in range(n):\n",
    "            output, hn = self.rnn(output, hn)\n",
    "       \n",
    "        assert len(output)<=self.batch_size,\"Invalid batchsize output\" #validate output\n",
    "        output=output.detach().numpy() #convert to numpy and reshape\n",
    "        output=np.sum(output,axis=1)[:,np.newaxis] #get the sum of the values\n",
    "        return torch.tensor(output[0][0])\n",
    "    def getAction(self,word):\n",
    "        vec=np.array(self.words.get(word,np.array([0,0,0,0]))) #give default\n",
    "        word_vec = torch.round(self.forward(vec)).detach().numpy() #gather vector\n",
    "        w=list(self.words.keys())[0]\n",
    "        for key in self.words:\n",
    "            if self.words[key].all()==word_vec.all(): #loop through\n",
    "                w=key\n",
    "        return w\n",
    "    def get_accuracy(self,words,labels): #get accuracy\n",
    "        acc=0\n",
    "        for word,label in zip(words,labels): #zip them together\n",
    "            act=self.getAction(word) #get action from this word\n",
    "            if act==label:\n",
    "                acc+=1\n",
    "        return acc/len(word) #return \n",
    "\n",
    "        \n",
    "net=R_Network(words_train)\n",
    "net.forward(x[0])\n",
    "net.train(words_train[0:10],x[0:10],epochs=100)\n",
    "print(\"Accuracy\",net.get_accuracy(words_train[0:10],x[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 719. MiB for an array with shape (9710, 9710) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\Computer Science\\Year 3\\Semester Two\\ANLE\\Coursework\\main.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000008?line=89'>90</a>\u001b[0m words_train\u001b[39m=\u001b[39mget_word_list(train_words)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000008?line=90'>91</a>\u001b[0m labels\u001b[39m=\u001b[39mget_label_as_data(words_train[\u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000008?line=91'>92</a>\u001b[0m netF\u001b[39m=\u001b[39mFNN(words_train,inp\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(words_train),hid\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(labels[\u001b[39m0\u001b[39;49m])\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m,out\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(labels[\u001b[39m0\u001b[39;49m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000008?line=92'>93</a>\u001b[0m \u001b[39m#print(netF.forward(x[0]))#\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000008?line=93'>94</a>\u001b[0m \u001b[39m#print(words_train[0:4],labels[0:4])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000008?line=94'>95</a>\u001b[0m netF\u001b[39m.\u001b[39mtrain(words_train[\u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m],labels[\u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m],epochs\u001b[39m=\u001b[39m\u001b[39m2000\u001b[39m)  \n",
      "\u001b[1;32md:\\Documents\\Computer Science\\Year 3\\Semester Two\\ANLE\\Coursework\\main.ipynb Cell 9'\u001b[0m in \u001b[0;36mFNN.__init__\u001b[1;34m(self, words, inp, hid, out, m, std)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000008?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwords\u001b[39m=\u001b[39m{}\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000008?line=8'>9</a>\u001b[0m \u001b[39m#vecs=convert_to_vector(words)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000008?line=9'>10</a>\u001b[0m vecs\u001b[39m=\u001b[39mget_label_as_data(words)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000008?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i,word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words): \u001b[39m#create dict\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000008?line=11'>12</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwords[word]\u001b[39m=\u001b[39mvecs[i]\n",
      "\u001b[1;32md:\\Documents\\Computer Science\\Year 3\\Semester Two\\ANLE\\Coursework\\main.ipynb Cell 6'\u001b[0m in \u001b[0;36mget_label_as_data\u001b[1;34m(labels)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000005?line=28'>29</a>\u001b[0m     a[i]\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000005?line=29'>30</a>\u001b[0m     lab\u001b[39m.\u001b[39mappend(a)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/Computer%20Science/Year%203/Semester%20Two/ANLE/Coursework/main.ipynb#ch0000005?line=30'>31</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49marray(lab)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 719. MiB for an array with shape (9710, 9710) and data type float64"
     ]
    }
   ],
   "source": [
    "class FNN:\n",
    "    def __init__(self,words,inp=4,hid=10,out=4,m=0,std=0.2): #feed forward neural network\n",
    "        \n",
    "        self.inp = torch.nn.Parameter(torch.normal(m,std,(inp, hid))) #output hidden\n",
    "        self.h0 = torch.nn.Parameter(torch.normal(m,std,(hid, hid))) #output hidden\n",
    "        #self.h1 = torch.nn.Parameter(torch.normal(m,std,(hid, hid))) #output hidden\n",
    "        self.h2 = torch.nn.Parameter(torch.normal(m,std,(hid, out))) #output hidden\n",
    "        self.words={}\n",
    "        #vecs=convert_to_vector(words)\n",
    "        vecs=get_label_as_data(words)\n",
    "        for i,word in enumerate(words): #create dict\n",
    "            self.words[word]=vecs[i]\n",
    "    def forward(self,item): #pass through network\n",
    "        item=item[:,np.newaxis]\n",
    "        x=torch.tensor(item).float()\n",
    "        x=(torch.mm(x.T, self.inp)) #torch.sigmoid\n",
    "        x=(torch.mm(x, self.h0))\n",
    "        #x=torch.sigmoid(torch.mm(x, self.h1))\n",
    "        x=(torch.mm(x, self.h2))\n",
    "        return x[0]\n",
    "    def train(self,words,targets,epochs=400):\n",
    "        print(\"Training...\")\n",
    "        #get each word in index\n",
    "        \n",
    "        #get words in relation of neigbours\n",
    "        #train word to next word\n",
    "        BestInp=None\n",
    "        Besth0=None\n",
    "        #Besth1=None\n",
    "        Besth2=None\n",
    "        BestAcc=0\n",
    "        optimizer = torch.optim.SGD((self.inp,self.h0,self.h2), lr=0.20) #, momentum=0.9\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        for epoch in range(epochs):\n",
    "            error=0\n",
    "            correct=0\n",
    "            for i in range(len(words)):\n",
    "                X=self.words.get(words[i],np.array([0,0,0,0]))\n",
    "                y=torch.tensor(targets[i]).double()\n",
    "                y=torch.tensor(y,requires_grad=True)\n",
    "               \n",
    "                pred=self.forward(X)\n",
    "                #X=torch.tensor(torch.tensor(X).double(),requires_grad=True)\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fn(pred, y)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()  \n",
    "                if np.argmax(pred.data.numpy())==i:\n",
    "                    correct+=1\n",
    "                error+=sum(abs(y.data.numpy() - pred.data.numpy()))/4\n",
    "            \n",
    "            if epoch %500 == 0:\n",
    "                #print(loss,torch.round(pred),y)\n",
    "                #print(sum((y.data.numpy() - pred.data.numpy()))/4)\n",
    "                print(\"Accuracy\",correct/len(words))\n",
    "                print(f\"Testing network @ epoch {epoch}\")\n",
    "                print(\"Error:\",error/len(words))\n",
    "        \n",
    "    def getWordThing(self,word):\n",
    "        return self.words.get(word,np.array([0,0,0,0]))\n",
    "    def get_action(self,word):\n",
    "        vec=self.words.get(word,np.array([0,0,0,0])) #give default\n",
    "        word_vec = self.forward(vec).data.numpy() #gather vector\n",
    "        ind=np.argmax(word_vec)\n",
    "        #print(word,word_vec,ind)\n",
    "        return list(self.words.keys())[ind]\n",
    "    def getWord(self,word_vec):\n",
    "        w=\"None\"\n",
    "        for key in self.words:\n",
    "            #print(self.words[key],word_vec)\n",
    "            if np.array_equal(self.words[key],word_vec): #loop through\n",
    "                w=key\n",
    "        return w\n",
    "    def get_accuracy(self,words,labels): #get accuracy\n",
    "        acc=0\n",
    "        for word,label in zip(words,labels): #zip them together\n",
    "            act=self.get_action(word) #get action from this word\n",
    "            print(act,list(self.words.keys())[np.argmax(label)])\n",
    "            if act==list(self.words.keys())[np.argmax(label)]:\n",
    "                acc+=1\n",
    "        return acc/len(words) #return accuracy\n",
    "\n",
    "\n",
    "words_train=get_word_list(train_words)\n",
    "labels=get_label_as_data(words_train[0:10])\n",
    "netF=FNN(words_train,inp=len(words_train),hid=len(labels[0])//2,out=len(labels[0]))\n",
    "#print(netF.forward(x[0]))#\n",
    "#print(words_train[0:4],labels[0:4])\n",
    "netF.train(words_train[0:10],labels[0:10],epochs=2000)  \n",
    "print(\"End Accuracy\",netF.get_accuracy(words_train[0:10],labels[0:10]))\n",
    "\n",
    "netF.getWordThing(words_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An attempt to make a feed forward neural network was placed in as there were issues with backpropagating the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the second model\n",
    "\n",
    "Using a cluster of n gram models we can predict likelihood of words based on different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram accuracy 0.5372217958557176 %\n",
      "Bigram accuracy 0.2403846153846154 %\n",
      "Trigram accuracy 0.08561643835616438 %\n",
      "Cluster Accuracy: 2.4769992922859165 %\n"
     ]
    }
   ],
   "source": [
    "class n_gram:\n",
    "    def __init__(self,sentences,n):\n",
    "        self.n=n\n",
    "        self.sent=sentences\n",
    "        #begin \n",
    "        self.data={}\n",
    "        self.format()\n",
    "    def format(self):\n",
    "        for sentence in self.sent:\n",
    "            words=sentence.split()\n",
    "            develop_n = []\n",
    "            for k in range(len(words)): #gather each word \n",
    "                i=0\n",
    "                s=\"\"\n",
    "                while i+k<len(words) and i<self.n: #get in terms of word connection\n",
    "                    s+=words[k+i].replace(\",\",\"\").replace(\";\",\"\").replace(\"*\",\"\").replace(\"'\",\"\").replace(\"#\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")+\"-\"\n",
    "                    i+=1\n",
    "                develop_n.append(s[:-1]) #add connections\n",
    "            for i,cont in enumerate(develop_n): #loop through number in sentence\n",
    "                if i+self.n+1<len(words): #validate next word\n",
    "                    nextword=words[i+self.n].replace(\",\",\"\").replace(\";\",\"\").replace(\"*\",\"\").replace(\"'\",\"\").replace(\"#\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "                    self.data[cont]=self.data.get(cont,{})\n",
    "                    self.data[cont][nextword]=self.data[cont].get(nextword,0)+1 #count\n",
    "    def get_predict(self,phrase): #get a prediction of a word\n",
    "        phrase=phrase.replace(\" \",\"-\")\n",
    "        words=self.data.get(phrase,None)\n",
    "        if words==None: return \"no word found\"\n",
    "        return {k: v for k, v in sorted(words.items(), key=lambda item: item[1])} #get return\n",
    "    def get_accuracy(self,words):\n",
    "        c=0\n",
    "        a=0\n",
    "        for sent in words: #loop through the sentences\n",
    "            sent=sent.replace(\",\",\"\").replace(\";\",\"\").replace(\"*\",\"\").replace(\"'\",\"\").replace(\"#\",\"\").replace(\"[\",\"\").replace(\"]\",\"\") #validate\n",
    "            sent=sent.split()\n",
    "            if len(sent)-2-self.n>self.n:              \n",
    "                num=random.randint(self.n,len(sent)-2-self.n) #get idx point\n",
    "                phrase=sent[num-self.n:num]\n",
    "                s=\"\"\n",
    "                for i in range(self.n): s+=phrase[i]+\"-\"\n",
    "                c+=1\n",
    "                pred=self.get_predict(s[:-1])\n",
    "                #print(phrase,list(pred.keys())[0])\n",
    "                if pred!=\"no word found\" and list(pred.keys())[0]==sent[num+1]: a+= 1 #correct prediction\n",
    "        return a/c\n",
    "unigram=n_gram(train_words.copy(),1)\n",
    "print(\"Unigram accuracy\",unigram.get_accuracy(train_words)*100,\"%\")\n",
    "bigram=n_gram(train_words.copy(),2)\n",
    "print(\"Bigram accuracy\",bigram.get_accuracy(train_words)*100,\"%\")\n",
    "trigram=n_gram(train_words.copy(),3)\n",
    "print(\"Trigram accuracy\",trigram.get_accuracy(train_words)*100,\"%\")\n",
    "def most_common(lst):\n",
    "    cur_length = 0\n",
    "    max_length = 0\n",
    "    cur_i = 0\n",
    "    max_i = 0\n",
    "    cur_item = None\n",
    "    max_item = None\n",
    "    for i, item in sorted(enumerate(lst), key=lambda x: x[1]):\n",
    "        if cur_item is None or cur_item != item:\n",
    "            if cur_length > max_length or (cur_length == max_length and cur_i < max_i):\n",
    "                max_length = cur_length\n",
    "                max_i = cur_i\n",
    "                max_item = cur_item\n",
    "            cur_length = 1\n",
    "            cur_i = i\n",
    "            cur_item = item\n",
    "        else:\n",
    "            cur_length += 1\n",
    "    if cur_length > max_length or (cur_length == max_length and cur_i < max_i):\n",
    "        return cur_item\n",
    "    return max_item\n",
    "def clusterPredict(sentence,mod1,mod2,mod3):\n",
    "    firstLast,secondLast,thirdLast=sentence[-1],sentence[-2],sentence[-3]\n",
    "    preds1,preds2,preds3=[mod1.get_predict(firstLast),mod2.get_predict(firstLast+\"-\"+secondLast),mod3.get_predict(firstLast+\"-\"+secondLast+\"-\"+thirdLast)]\n",
    "    words=[]\n",
    "    multiplier=1\n",
    "    for dict in [preds1,preds2,preds3]: #loop through all predictions\n",
    "        if dict!=\"no word found\":\n",
    "            for key in dict:\n",
    "                for i in range(dict[key]*multiplier):\n",
    "                    words.append(key) #add multiple times if occurs multiple \n",
    "        multiplier+=1\n",
    "    return most_common(words) #get most common word in list\n",
    "acc=0\n",
    "num=len(train_words)\n",
    "for i in range(num):\n",
    "    sent=train_words[i].split()\n",
    "    if len(sent)>=4:\n",
    "        pred=clusterPredict(sent[0:-1],unigram,bigram,trigram)\n",
    "        if sent[-1]==pred: acc+=1\n",
    "print(\"Cluster Accuracy:\",acc/num *100,\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4dd024378a2eab6b61421bb2db15dae0e0d8b99b3e2b4f86d231a4685d0f22e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
